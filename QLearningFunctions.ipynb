{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviornments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('states high value:')\n",
    "#print(env.observation_space.high[0]) #4.8  position of cart\n",
    "#print(env.observation_space.high[1])  #3.4028235e+38   velocity of cart\n",
    "#print(env.observation_space.high[2])  #0.41887903   angle of pole\n",
    "#print(env.observation_space.high[3])  #3.4028235e+38  rotation rate of pole\n",
    "\n",
    "#print('states low value:')\n",
    "#print(env.observation_space.low[0])   #-4.8\n",
    "#print(env.observation_space.low[1])  #-3.4028235e+38\n",
    "#print(env.observation_space.low[2])  #-0.41887903\n",
    "#print(env.observation_space.low[3])  #-3.4028235e+38\n",
    "# [position of cart, velocity of cart, angle of pole, rotation rate of pole]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('states high value:')\n",
    "#print(env.observation_space.high[0]) # 1  cos of first rotational joint \n",
    "#print(env.observation_space.high[1])  # 1 sin of first rotational joint\n",
    "#print(env.observation_space.high[2])  # 1 cos of first rotational joint\n",
    "#print(env.observation_space.high[3])  # 1 sin of first rotational joint\n",
    "#print(env.observation_space.high[4])  # 12.566371 angular velocity of 1st joint\n",
    "#print(env.observation_space.high[5])  # 28.274334 angular velocity of 2nd joint\n",
    "#[cos(theta1), sin(theta1), cos(theta2), sin(theta2), theta_dot1, theta_dot2]\n",
    "\n",
    "#Acrobot does not have threshold at which it is considered solved.\n",
    "\n",
    "\"\"\"\n",
    "    Acrobot is a 2-link pendulum with only the second joint actuated.\n",
    "    Initially, both links point downwards. The goal is to swing the\n",
    "    end-effector at a height at least the length of one link above the base.\n",
    "    Both links can swing freely and can pass by each other, i.e., they don't\n",
    "    collide when they have the same angle.\n",
    "    **STATE:**\n",
    "    The state consists of the sin() and cos() of the two rotational joint\n",
    "    angles and the joint angular velocities :\n",
    "    [cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2].\n",
    "    For the first link, an angle of 0 corresponds to the link pointing downwards.\n",
    "    The angle of the second link is relative to the angle of the first link.\n",
    "    An angle of 0 corresponds to having the same angle between the two links.\n",
    "    A state of [1, 0, 1, 0, ..., ...] means that both links point downwards.\n",
    "    **ACTIONS:**\n",
    "    The action is either applying +1, 0 or -1 torque on the joint between\n",
    "    the two pendulum links.\n",
    "    \n",
    "    This is an unsolved environment, which means it does not have a specified \n",
    "    reward threshold at which it’s considered solved.\n",
    "\"\"\"\n",
    "\"\"\"This environment consists of a two link, two joint robot.\n",
    "The joint between the links is actuated. At the start of the episode,\n",
    "the links are hanging downwards. At every timestep the agent chooses\n",
    "an action that correspond to applying a force to move the actuated\n",
    "link to the right, to the left, or to not applying a force.\n",
    "The episode is over once the end of the lower link swings above a certain\n",
    "height.The goal is to end the episode in the fewest possible timesteps.\n",
    "We use the OpenAI Gym ‘Acrobot-v1’ environment. This implementation is\n",
    "based on the system presented in (Geram- ifard et al., 2015).\n",
    "Each observation is a set consisting of readings from six sensors,\n",
    "corresponding to the rotational joint angles and velocities of joints and links.\n",
    "The action space is discrete with three elements, and at each timestep\n",
    "the environment returns the observation and a reward of -1. \n",
    "An episode is terminated after 200 time steps irrespective of\n",
    "the state of the robot. This is an unsolved environment,\n",
    "which means it does not have a specified reward threshold at which\n",
    "it’s considered solved.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning():\n",
    "    def __init__(self,bins=(1, 1, 6, 12), max_episodes = 1000, max_steps = 195, min_step_size=0.1,min_epsilon=0.0, gamma = 1.0 , decay = 25, action_space = env.action_space.n, env = gym.make('CartPole-v0')):   #max_env_steps=None\n",
    "        self.bins = bins #(1, 1, 6, 12)--> scale theta and theta' to the intervals (0,6) and (0,12) respectively \n",
    "        #rather than 3 buckets for x and xdot, making it 1 makes it so that the state space dimentionalty is signifigantly reduced \n",
    "        #Since we assign only one bucket to the cart position and velocity, it means that we ignore these 2 values.\n",
    "        self.max_episodes = max_episodes # number of training episodes \n",
    "        self.max_steps = max_steps # average ticks over 100 episodes required for win\n",
    "        self.min_step_size = min_step_size  #step size\n",
    "        self.min_epsilon = min_epsilon #  \n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.decay = ada_divisor\n",
    "        self.action_space = action_space\n",
    "        self.env = env\n",
    "        self.QTable = np.zeros((self.buckets + (self.action_space,))) #The Q LookUp Table --> its dimension is  (1 x 1 x 6 x 12 x 2) \n",
    "        # or self.QTable = np.empty((self.bins + (self.action_space,)))\n",
    "        self.actions = None\n",
    "        self.tau = 500    # tau --> 0 pure exploitation tau --> infinity pure exploration\n",
    "        self.tau_decay = False\n",
    "        self.decay_factor_tau = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article: \"When I was staring at the animation, I noticed that the cart didn’t really move out of bound that often.\n",
    "Also, I only needed to balance the pole for 200 time steps. It typically didn’t drift that far while balancing the pole.\n",
    "Therefore, I removed the x and x_dot components and leaving only the theta, and theta_dot components.\n",
    "With this significantly reduced state-space, I was able to solve the problem much quicker in only 136 episodes !--> why (1,1,...)\"\n",
    "#def __init__(self, actions=None, tau=500, tau_decay=False, decay_factor_tau=0.1):\n",
    "#assert tau >= 0, \n",
    "#assert bool == type(tau_decay),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def discretize(self, state):\n",
    "        # observation = obs, reward, done, info = self.env.step(action)--> in training\n",
    "        # define upper and lower bounds for each state value\n",
    "        #it is necessary to give finite bounds and reduce the range of the cart velocity and pole velocity at tip – i.e. [-0.5, +0.5] and [-50, +50],\n",
    "        #respectively – in order to be able to discretise.\n",
    "        upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]  \n",
    "        lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n",
    "        descretizedList = list()\n",
    "        for i in  range(len(state)): #state = tuple (0,0,X,X)--> aka the state space  #scaling factor\n",
    "            ratios = (state[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) # (state+lower / bin_width) = bucket size\n",
    "            new_obs = int(round((self.bins[i] - 1) * ratios)) # makes the first two buckets 0, and #bucket_lower := min_x + bucket × bucket size\n",
    "            new_obs = min(self.bins[i] - 1, max(0, new_obs)) #no matter what the buckets-1 is going to be 0 or greater than 0\n",
    "            #this makes sure that the first two components of the tuple is 0\n",
    "            #why are we taking the min between the bucket-1 and the new_obs\n",
    "            descretizedList.append(new_obs)\n",
    "        return tuple(descretizedList)\n",
    "\n",
    "#print(env.observation_space.high[2])  #0.41887903   angle of pole\n",
    "#print(env.observation_space.high[3])  #3.4028235e+38  rotation rate of pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose those values for [1], and [3] for buckets as I was inspired by a blog post, muetsch and tuzzer--> I tried it with other values and it did not converge\n",
    "https://ferdinand-muetsch.de/cartpole-with-qlearning-first-experiences-with-openai-gym.html\n",
    "https://mc.ai/openai-gyms-cart-pole-balancing-using-q-learning/\n",
    "'''When I was staring at the animation, I noticed that the cart didn’t really move out of bound that often.\n",
    "Also, I only needed to balance the pole for 200 time steps. It typically didn’t drift that far while balancing the pole.\n",
    "Therefore, I removed the x and x_dot components and leaving only the theta, and theta_dot components.\n",
    "With this significantly reduced state-space, I was able to solve the problem much quicker in only 136 episodes... why (1,1,...)\n",
    "(1, 1, 6, 12)--> number of buckets for each action space rather than 3 buckets for x and xdot, making it 1 makes it so that the state space dimentionalty is signifigantly reduced\n",
    "I also tried [1] and [3] to be 3.4 as its true value is 3.4028235e+38-> this did not work well however\n",
    "\n",
    "As states are continuous, discretize them into buckets--> Binning or discretization is the process of transforming numerical variables into categorical counterparts. An example is to bin values for Age into categories such as 20-39, 40-59, and 60-79. Numerical variables are usually discretized in the modeling methods based on frequency tables (e.g., decision trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def discretize(self, state):\n",
    "        # observation = obs, reward, done, info = self.env.step(action)--> in training\n",
    "        # define upper and lower bounds for each state value\n",
    "        #it is necessary to give finite bounds and reduce the range of the cart velocity and pole velocity at tip – i.e. [-0.5, +0.5] and [-50, +50],\n",
    "        #respectively – in order to be able to discretise.\n",
    "        upper_bounds = [1.0, 1.0, 1.0, 1.0, env.observation_space.high[4], env.observation_space.high[5]]  \n",
    "        lower_bounds = [-1.0,-1.0,-1.0,-1.0, env.observation_space.low[4], env.observation_space.low[5]]\n",
    "        descretizedList = list()\n",
    "        for i in  range(len(state)): #state = tuple (0,0,X,X)--> aka the state space  #scaling factor\n",
    "            ratios = (state[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) # (state+lower / bin_width) = bucket size\n",
    "            new_obs = int(round((self.bins[i] - 1) * ratios)) # makes the first two buckets 0, and #bucket_lower := min_x + bucket × bucket size\n",
    "            new_obs = min(self.bins[i] - 1, max(0, new_obs)) #no matter what the buckets-1 is going to be 0 or greater than 0\n",
    "            #this makes sure that the first two components of the tuple is 0\n",
    "            #why are we taking the min between the bucket-1 and the new_obs\n",
    "            descretizedList.append(new_obs)\n",
    "        return tuple(descretizedList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavior Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy: Behavior Policy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def action(self, state, epsilon):\n",
    "        ''' choose an action using the epsilon policy '''\n",
    "        #exploration-exploitation = np.random.random()\n",
    "        #best = np.argmax(self.QTable[state])\n",
    "        #random = env.action_space.sample()  # select a random action (see https://github.com/openai/gym/wiki/CartPole-v0) # exploration\n",
    "        if np.random.random() <= epsilon:\n",
    "            action = self.env.action_space.sample()   \n",
    "        else:\n",
    "            action = np.argmax(self.QTable[state])  # exploitation\n",
    "            #print(\"best state\", np.argmax(self.QTable[state]))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epsilon decay inspired by--> Hands-On Intelligent Agents with OpenAI Gym: Your guide to developing AI using Deep Reinforcment Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            action = random.choice(self.env.actions)   \n",
    "        else:\n",
    "            action = np.argmax(self.QTable[state])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Behavior Policy: Behavior Policy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def action(self, state, epsilon):\n",
    "        action = self.env.action_space.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boltzmann Exploration Policy: Behavior Policy 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1612.05628.pdf\n",
    "    def Boltzmann(self, state, tau):\n",
    "        qList = self.QTable[state] #moves = self.q_matrix[pos_index]\n",
    "        self.actions = [i for i in range(0, env.action_space.n)]\n",
    "        # Circumvent math issues with temperature actually being 0\n",
    "        if self.tau > 0.0001:\n",
    "            #print(self.tau)\n",
    "            # Compute action probabilities using tau; when\n",
    "            # tau is high, we're treating values of very different\n",
    "            # Q-values as more equally choosable\n",
    "            action_probs_numes = []\n",
    "            denom = 0\n",
    "            for q in qList:\n",
    "                val = math.exp(q / self.tau)\n",
    "                action_probs_numes.append(val)\n",
    "                denom += val  # summation of the values\n",
    "            action_probs = []\n",
    "            for x in action_probs_numes:\n",
    "                probs = x / denom \n",
    "                action_probs.append(probs)    \n",
    "            # Pick random move, in which moves with higher probability are\n",
    "            # more likely to be chosen, but it is obviously not guaranteed\n",
    "            prob_sum = 0\n",
    "            #action = action_probs.index(max(action_probs))\n",
    "            #print(action)\n",
    "            #action = np.argmax(action_probs)\n",
    "            action = random.choices(self.actions, weights=action_probs, k=1)[0] # the weights give a weight to correspond \n",
    "            #for each action , and then chooses a random action with each action weighted by it prob. and then chooses\n",
    "            #I attemped using the action for the max probabilty but it only returned one action each time\n",
    "        else:\n",
    "            # Here, we're totally cold; meaning, we're just exploiting\n",
    "            action = np.argmax(self.QTable[state])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: In exploration, we would ideally like to exploit all the information present in the estimated Q-values produced by our network. Boltzmann exploration does just this. Instead of always taking the optimal action, or taking a random action, this approach involves choosing an action with weighted probabilities. To accomplish this we use a softmax over the networks estimates of value for each action. In this case the action which the agent estimates to be optimal is most likely (but is not guaranteed) to be chosen. The biggest advantage over e-greedy is that information about likely value of the other actions can also be taken into consideration. If there are 4 actions available to an agent, in e-greedy the 3 actions estimated to be non-optimal are all considered equally, but in Boltzmann exploration they are weighed by their relative value. This way the agent can ignore actions which it estimates to be largely sub-optimal and give more attention to potentially promising, but not necessarily ideal actions.\n",
    "Adjusting during training: In practice we utilize an additional temperature parameter (τ) which is annealed over time. This parameter controls the spread of the softmax distribution, such that all actions are considered equally at the start of training, and actions are sparsely distributed by the end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/lkgwbr/QLearnGrid/blob/5955dc4241c50896f7ae42b155738b25014f15f3/main.py#L196\n",
    "https://github.com/lucadivit/Reinforcement_Learning_Maze_Solver/tree/b4c56aec5f6aee56cbf7f5b77d4ec7424f40fb14\n",
    "Inspired by these codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def Q_Update(self, state, action, reward, new_state, step_size):\n",
    "        ''' Update the Q-values'''\n",
    "        # Qk+1(s,a) = Qk(s,a) + alpha[R(s, a, s')+ gamma(maxQk(s,a,))- Qk(s,a))\n",
    "        self.QTable[state][action] += step_size * (reward + self.gamma * (np.max(self.QTable[new_state])) - self.QTable[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural log based decay- decays in begining and is constant all the way through the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decay_epsilon(self, t):  \n",
    "        ##### DIMINISHING EPSILON ####\n",
    "        #print(max(self.min_alpha, min(1.0, 1.0 - math.log10((t + 1) / self.ada_divisor))))\n",
    "        #logarithmic model -->  a + b ln(x)- this is increasing a-blnx- decreasing\n",
    "        #1- log bc max alpha is 1 and and you have to diminish it from there until you get to the minimum epsilon \n",
    "        return max(self.min_epsilon, min(1, 1.0 - math.log((t + 1) / self.decay), math.e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attemped several different decay function--> linear, and exponential, and logbase 10, however, I settled on natural log as it worked really well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constant decay- decay every episode by a factor of 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decay_epsilon(self):\n",
    "        epsilon = self.epsilon * 0.99\n",
    "        if epsilon < self.min_epsilon:\n",
    "            self.epsilon = self.min_epsilon\n",
    "        else:\n",
    "            self.epsilon = epsilon\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decay_epsilon(self):\n",
    "        epsilon = self.epsilon - 0.001\n",
    "        if epsilon < self.min_epsilon:\n",
    "            self.epsilon = self.min_epsilon\n",
    "        else:\n",
    "            self.epsilon = epsilon\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Alpha Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decay_step_size(self, t):\n",
    "        ##### DIMINISHING STEP SIZES ####\n",
    "        #print(max(self.min_alpha, min(1.0, 1.0 - math.log10((t + 1) / self.ada_divisor))))\n",
    "        #logarithmic model -->  a + b ln(x)- this is increasing a-blnx- decreasing\n",
    "        #1- log bc max alpha is 1 and and you have to diminish it from there until you get to the minimum alpha \n",
    "        return max(self.min_step_size, min(1.0, 1.0 - math.log((t + 1) / self.decay), math.e))  #makes sure it doesn't go above 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attemped several different decay function--> linear, and exponential, and logbase 10, however, I settled on natural log as it worked really well. In the code for constant alpha I used 0.9. I attemped several different values ranging from 0.1 to 1, however they all preformed relatively similarly--> I am still trying to understand why because it should still work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decay_step_size(self, t):\n",
    "        if t == 0:\n",
    "            step_size = 1.0\n",
    "        elif t > 0:\n",
    "            step_size = self.alpha / (t ** self.beta)  \n",
    "        return step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fulfills the requirement of non summable but square summable. \n",
    "\n",
    "alpha and beta are the hyperparameters and bounded by the ranges as shown above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decay_step_size(self, tt, e, t):\n",
    "        num = self.alpha\n",
    "        dem = tt**self.beta\n",
    "        if e == 0 and t == 0:\n",
    "            step_size = 1\n",
    "        elif tt > 0:\n",
    "            step_size = num / dem\n",
    "        return step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def moving_average(self, rewards, window_size = 10): #A specified window size determines the size of each subset.\n",
    "        #In statistics, a moving average (rolling average or running average)\n",
    "        #is a calculation to analyze data points by creating a series of averages of different subsets of the full data set. \n",
    "        #The moving average is calculated by adding the rewards over a certain period \n",
    "        #and dividing the sum by the total number of periods.\n",
    "        sum_vec = np.cumsum(np.insert(rewards, 0, 0))  \n",
    "        moving_ave = (sum_vec[window_size:] - sum_vec[:-window_size]) / window_size  #formula for moving average\n",
    "        return moving_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pseudo code:\n",
    "Initialize Q(s,a) arbitrarily\n",
    "Repeat (for each generation):\n",
    "Initialize state s\n",
    "   While (s is not a terminal state):\n",
    "   Choose a from s using policy derived from Q\n",
    "        Take action a, observe r, s'\n",
    "        Q(s,a) += alpha * (r + gamma * max,Q(s') - Q(s,a))\n",
    "        s = s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def learning(self):\n",
    "        rewards = []\n",
    "        for e in range(self.max_episodes):  \n",
    "            # As states are continuous, discretize them into buckets\n",
    "            state = self.discretize(self.env.reset())  # for each episode the enviornment resets and discretizes the state\n",
    "            epsilon = self.decay_epsilon(e)\n",
    "            step_size = self.decay_step_size(e)\n",
    "            i = 0\n",
    "            for t in range(self.max_steps):   #for every timestep\n",
    "                action = self.action(state, epsilon)\n",
    "                new_state, reward, done, info = self.env.step(action)  #the obeservation space (0,0,x,x)\n",
    "                new_state = self.discretize(new_state)\n",
    "                self.Q_Update(state, action, reward, new_state, step_size)\n",
    "                state = new_state\n",
    "                i += 1\n",
    "                if done:\n",
    "                    break\n",
    "            rewards.append(i)\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def learning(self):\n",
    "        rewards = []\n",
    "        alphaList = []\n",
    "        taus = []\n",
    "        times = 0\n",
    "        listof50 = deque(maxlen=50)\n",
    "        #episodes = []\n",
    "        # this part until the end of the while loop:\n",
    "        # this is the part of the function that trains the enviornment.\n",
    "        for e in range(self.max_episodes):  \n",
    "            # As states are continuous, discretize them into buckets\n",
    "            # for each episode the enviornment resets and discretizes the state\n",
    "            state = self.discretize(self.env.reset())\n",
    "            #epsilon = self.epsilon\n",
    "            tau = self.decay_tau(e)\n",
    "            #print(\"epsilon\", epsilon)\n",
    "            step_size = self.decay_step_size(e)\n",
    "            #print(\"alpha\", alpha)\n",
    "            i = 0\n",
    "            for t in range(self.max_steps):\n",
    "                #step_size = self.decay_step_size(e)\n",
    "                #self.env.render()\n",
    "                if e%2==0:\n",
    "                    action = self.Boltzmann(state, tau) #, epsi\n",
    "                elif e%2==1:\n",
    "                    action = self.action_policy(state, policy)\n",
    "                new_state, reward, done, info = self.env.step(action)\n",
    "                #print(reward)\n",
    "                new_state = self.discretize(new_state)\n",
    "                self.Q_Update(state, action, reward, new_state, step_size)\n",
    "                state = new_state\n",
    "                i += reward  #-1 for each timestep the agent takes to swing up  # reward threshold -100\n",
    "                if done:\n",
    "                    break\n",
    "            stateTuple = list()\n",
    "            stateList = []\n",
    "            actionsList = []\n",
    "            states = []\n",
    "            for list1 in self.QTable:\n",
    "                for nextList in list1:\n",
    "                    for nextList1 in nextList:\n",
    "                        for nextList2 in nextList1:\n",
    "                            for nextList3 in nextList2:\n",
    "                                for nextList4 in nextList3:\n",
    "                                    actions = np.argmax(nextList4)\n",
    "                                    actionsList.append(actions)\n",
    "            observations = np.argwhere(self.QTable >= -600)\n",
    "            for obsList in observations:\n",
    "                stateTuple = (obsList[0], obsList[1], obsList[2], obsList[3], obsList[4], obsList[5])\n",
    "                stateList.append(stateTuple)\n",
    "            del stateList[1::3]\n",
    "            del stateList[1::2]\n",
    "            #print(len(actionsList), len(stateList))\n",
    "            policy = dict(zip(stateList, actionsList))\n",
    "            #print(policy)\n",
    "            if e%2 ==1:\n",
    "                rewards.append(i)\n",
    "                alphaList.append(step_size)\n",
    "                taus.append(tau)\n",
    "                listof50.append(i)\n",
    "                mean_listof50 = np.mean(listof50)\n",
    "                if i >= -100:\n",
    "                    times+=1\n",
    "                #print(\"Solved after \" + str(e) + \" episodes.\")\n",
    "                #print(str(e) + \" episodes: \" + str(i) + \" reward\")\n",
    "                #return rewards, alphaList, epsilonList\n",
    "        print(times)\n",
    "        return rewards, alphaList, taus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def learning(self):\n",
    "        rewards = []\n",
    "        alphaList = []\n",
    "        epsilonList = []\n",
    "        #episodes = []\n",
    "        # this part until the end of the while loop:\n",
    "        # this is the part of the function that trains the enviornment.\n",
    "        for e in range(self.max_episodes):  \n",
    "            if e % 10000 == 0:\n",
    "                print(e)\n",
    "            #state1 = np.random.randint(0,9) #self.env.reset()\n",
    "            epsilon = self.decay_epsilon() #e)\n",
    "            step_size = 0.01 #self.decay_step_size(e)\n",
    "            for state in self.env.states:\n",
    "                for action in self.env.actions:\n",
    "                    transitionVector = self.env.transitions[action][state]\n",
    "                    new_state = random.choices(population=self.env.states, weights=list(transitionVector))\n",
    "                    new_state = new_state[0]\n",
    "                    reward = self.env.rewards[action][state] #[new_state]\n",
    "                    self.Q_Update(state, action, reward, new_state, step_size)\n",
    "            rewards.append(np.linalg.norm(self.QTable-self.QFunction))#averageQ)\n",
    "            alphaList.append(step_size)\n",
    "            epsilonList.append(epsilon)\n",
    "            mean_rewards =  np.mean(rewards)\n",
    "        print(self.QTable)\n",
    "        #print(self.V)\n",
    "        return rewards, alphaList, epsilonList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code only works for the MDP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Policy Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I starts with this optimal policy would I get the right rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            if mean_listof100 >= self.max_steps and e >= 100:\n",
    "                stateTuple = list()\n",
    "                stateList = []\n",
    "                actionsList = []\n",
    "                states = []\n",
    "                #print(self.QTable)\n",
    "                for list1 in self.QTable:\n",
    "                    for nextList in list1:\n",
    "                        #print(nextList)\n",
    "                        #print(np.argwhere(nextList>=0))\n",
    "                        for nextList1 in nextList:\n",
    "                            #print(nextList1)\n",
    "                            #print(np.argwhere(nextList1>=0)) \n",
    "                            for nextList2 in nextList1:\n",
    "                                actions = np.argmax(nextList2)\n",
    "                                #print(actions)\n",
    "                                actionsList.append(actions)\n",
    "                                #print(nextList2)\n",
    "                observations = np.argwhere(self.QTable>=0)\n",
    "                j = 0\n",
    "                for obsList in observations:\n",
    "                    stateTuple = (obsList[0], obsList[1], obsList[2], obsList[3])\n",
    "                    stateList.append(stateTuple)\n",
    "                del stateList[1::2]\n",
    "                policy = dict(zip(stateList, actionsList))\n",
    "                print(policy)\n",
    "                #print(\"All states:\", stateList)\n",
    "                #print(\"optimal actions:\", actionsList)\n",
    "                print(\"Solved after \" + str(e) + \" episodes.\")\n",
    "                return rewards, alphaList, epsilonList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            stateTuple = list()\n",
    "            stateList = []\n",
    "            actionsList = []\n",
    "            states = []\n",
    "            #print(self.QTable)\n",
    "            for list1 in self.QTable:\n",
    "                for nextList in list1:\n",
    "                    #print(nextList)\n",
    "                    #print(np.argwhere(nextList>=0))\n",
    "                    for nextList1 in nextList:\n",
    "                        #print(nextList1)\n",
    "                        #print(np.argwhere(nextList1>=0)) \n",
    "                        for nextList2 in nextList1:\n",
    "                            for nextList3 in nextList2:\n",
    "                                for nextList4 in nextList3:\n",
    "                                    actions = np.argmax(nextList4)\n",
    "                                    #print(actions)\n",
    "                                    actionsList.append(actions)\n",
    "            observations = np.argwhere(self.QTable>=-500)\n",
    "            #print(observations)\n",
    "            j = 0\n",
    "            for obsList in observations:\n",
    "                stateTuple = (obsList[0], obsList[1], obsList[2], obsList[3], obsList[4], obsList[5])\n",
    "                stateList.append(stateTuple)\n",
    "            del stateList[1::3]\n",
    "            del stateList[1::2]\n",
    "            policy = dict(zip(stateList, actionsList))\n",
    "            #print(policy)\n",
    "            #print(e, np.argmax(self.QTable))\n",
    "            listof100.append(i)\n",
    "            mean_listof100 = np.mean(listof100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea of Model-based RL is using the model and the cost function to locate the optimal path of actions (to be exact — a trajectory of states and actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ''' Run main program. '''\n",
    "    #solve = Qlearning()\n",
    "    #rewards, alpha, epsilon = learning(solve, max_episodes = 500, bins=(1, 1, 6, 12), max_steps = 200,min_alpha=0.1, min_epsilon=0.1, epsilon = 1.0, gamma = 1.0, decay = 25, solved=False, action_space = env.action_space.n)\n",
    "    trajectory = [] \n",
    "    listof_195 = []\n",
    "    for episode in range(500):\n",
    "        listof_195.append(195)\n",
    "    for i in range(100):\n",
    "        solve = Qlearning()\n",
    "        rewards, alpha, epsilon = learning(solve)\n",
    "        trajectory.append(rewards)\n",
    "        #print(rewards)\n",
    "        #print(\"alpha\", alpha)\n",
    "        print(i)\n",
    "    a = np.array(trajectory)\n",
    "    final = np.mean(a, axis=0)\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    plt.plot(final, \n",
    "            color = 'b',\n",
    "            label = 'Expected Rewards')\n",
    "    plt.plot(listof_195,  # epsilon_hist,\n",
    "             color='r',\n",
    "             label='195')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('CartPole-v0: Expected rewards')\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('expected rewards') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The finite and infinite MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite Horizon MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDP:\n",
    "    def __init__(self,s,a):\n",
    "        self.num_state             = s\n",
    "        self.num_action            = a\n",
    "        self.states                = np.array(range(0,s))\n",
    "        self.actions               = np.array(range(0,a))\n",
    "        self.transitions           = np.zeros((a,s,s))\n",
    "        self.rewards               = np.zeros((a,s,s))\n",
    "        #print(rewards)\n",
    "        \n",
    "# The function below initializes transition probability matrix and rewards marks \n",
    "\n",
    "    def initialize_mdp(self):      \n",
    "        np.random.seed(0)        #for reproducibility \n",
    "        self.transitions, self.rewards         = mdptoolbox.example.rand(self.num_state,self.num_action)\n",
    "        self.rewards                           = np.random.rand(self.num_action, self.num_state) #   ( self.num_state,self.num_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a MDP created with the mdptoolboox. This is specifically code taken from https://github.com/gt-coar/NesaraREU20/blob/master/Value_Iteration.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Horizon MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was inspired with nesara's code as I used her code to create and print the the rewards and transitions for an 11 by 3 mdp, and then manually changed the transitions and rewards to make the 11th state an absorbing state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning(MarkovDP):\n",
    "    def __init__(self, env, max_episodes = 350000, max_steps = 1,min_alpha=0.0, min_epsilon= 0.1, epsilon = 1.00, gamma = 0.95, decay = 25, solved=False):   #max_env_steps=None\n",
    "        self.max_episodes = max_episodes # number of training episodes \n",
    "        self.max_steps = max_steps # average ticks over 100 episodes required for win\n",
    "        self.alpha = 1.0 # learning rate parameter\n",
    "        self.beta = 0.5 # learning rate parameter\n",
    "        self.epsilon = epsilon # exploration rate\n",
    "        self.min_alpha = min_alpha\n",
    "        self.min_epsilon = min_epsilon \n",
    "        self.solved = solved\n",
    "        self.gamma = gamma\n",
    "        self.decay = decay  #works for most numbers between 5 and 35\n",
    "        self.env = env #gym.make('CartPole-v0')\n",
    "        self.QTable = np.ones((env.num_state, env.num_action))\n",
    "        self.zeros = np.full((10,3), 0)\n",
    "        self.QFunction = np.array([[13.23176522, 13.33925622, 14.05469772], [13.52671959, 13.1224803,  13.22367197], [13.30159516, 13.86810447, 13.51346527], [13.66552722, 13.47151662, 13.23704243], [14.00228974, 13.41027839, 13.44004462], [13.42221627, 13.84567785, 13.56730379], [13.17987508, 13.49701082, 13.59308849], [13.24370111, 13.86450877, 13.89669707], [13.86681663, 13.87073118, 13.73493382], [13.24989143, 13.5198381,  13.37349781]])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the biggest difference from the openAIGym code is how the env is definedit is through initialize mdp) and self.QFunction which is the optimal Q function found through value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def learning(self):\n",
    "        QList = []\n",
    "        QLog = []\n",
    "        state = np.random.randint(0, self.env.num_state)\n",
    "        tt=0\n",
    "        r = 0\n",
    "        q = 0\n",
    "        for e in range(self.max_episodes):  \n",
    "            if e % 100000 == 0:\n",
    "                print(e)\n",
    "            epsilon = self.decay_epsilon() #e)\n",
    "            step_size = 0.01 #self.decay_step_size(e)\n",
    "            action = self.action(state, epsilon)\n",
    "            transitionVector = self.env.transitions[action][state]\n",
    "            new_state = random.choices(population=self.env.states, weights=list(transitionVector))\n",
    "            new_state = new_state[0]\n",
    "            reward = self.env.rewards[action][state]\n",
    "            self.Q_Update(state, action, reward, new_state, step_size)\n",
    "            state = new_state\n",
    "            QList.append(np.linalg.norm(self.QTable - self.QFunction))\n",
    "            QLog.append(math.log10(np.linalg.norm(self.QTable - self.QFunction)))\n",
    "        return QList, QLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for the infinite MDP as it is not epsisodic. The new state is chosen based on the state transition probabilities ( the choice is weighted by the probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
